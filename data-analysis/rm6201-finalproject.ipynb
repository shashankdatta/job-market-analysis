{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cb01bd-85e1-48c7-9cda-bffec5b5f5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "######   feature 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a59fd0bd-1e38-4f57-989f-12030f291418",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/23 10:52:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Job Skills Analysis\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1db8ad2c-5f1e-48dd-a415-9c956be9f955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"job_descriptions.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfa94c29-c719-4c67-ade7-cf5c613d7691",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df.filter(df[\"Role\"].like(\"%Interaction Designer%\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4707b7e2-3797-4d19-bd9e-47e1f31598d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, split, lower,col,desc\n",
    "\n",
    "# Splitting the skills string into individual skills and converting to lowercase\n",
    "exploded_skills = filtered_df.withColumn(\"Skill\", explode(split(lower(col(\"Skills\")), \", \")))\n",
    "\n",
    "# Grouping by skills and counting occurrences\n",
    "skills_count = exploded_skills.groupBy(\"Skill\").count()\n",
    "\n",
    "# Sorting by the most frequently mentioned skills\n",
    "most_common_skills = skills_count.orderBy(desc(\"count\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efe1d04f-27ca-4e6b-a7b7-b7c5c7075c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:====================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               Skill|count|\n",
      "+--------------------+-----+\n",
      "|javascript fronte...| 3563|\n",
      "|               react| 3563|\n",
      "|                html| 3563|\n",
      "|                 css| 3563|\n",
      "|angular) user exp...| 3563|\n",
      "|python) database ...| 3452|\n",
      "|             node.js| 3452|\n",
      "|                 sql| 3452|\n",
      "|server-side progr...| 3452|\n",
      "|            mongodb)| 3452|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "most_common_skills.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00875cdd-e8c0-453b-b86f-93183692f8b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m col, explode, lower, regexp_replace, split\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Normalize and clean up the skills text\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Replace specific punctuations with a comma, handle and normalize spaces\u001b[39;00m\n\u001b[1;32m      5\u001b[0m normalized_skills_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormalized_skills\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m----> 6\u001b[0m     \u001b[43mregexp_replace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSkill\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43ms+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m), \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnormalized_skills\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Splitting the 'normalized_skills' column on commas and exploding into individual skills\u001b[39;00m\n\u001b[1;32m     10\u001b[0m exploded_skills \u001b[38;5;241m=\u001b[39m normalized_skills_df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkill\u001b[39m\u001b[38;5;124m\"\u001b[39m, explode(split(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormalized_skills\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, explode, lower, regexp_replace, split\n",
    "\n",
    "# Normalize and clean up the skills text\n",
    "# Replace specific punctuations with a comma, handle and normalize spaces\n",
    "normalized_skills_df = df.withColumn(\"normalized_skills\", \n",
    "    regexp_replace(col(\"Skill\"), r\"\\)\\s+\", \"), \").replace([\"(\", \")\"], \"\", \"normalized_skills\")\n",
    ")\n",
    "\n",
    "# Splitting the 'normalized_skills' column on commas and exploding into individual skills\n",
    "exploded_skills = normalized_skills_df.withColumn(\"Skill\", explode(split(col(\"normalized_skills\"), \",\\s*\")))\n",
    "\n",
    "# Further cleaning to trim spaces and convert to lowercase\n",
    "cleaned_skills = exploded_skills.withColumn(\"Skill\", lower(trim(col(\"Skill\"))))\n",
    "\n",
    "# Grouping by skills and counting occurrences\n",
    "skills_count = cleaned_skills.groupBy(\"Skill\").count()\n",
    "\n",
    "# Sorting by the most frequently mentioned skills\n",
    "most_common_skills = skills_count.orderBy(col(\"count\").desc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "991cb5c2-06a6-41db-adbd-a6f2f2588f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode, lower, trim, regexp_replace, split\n",
    "# This replaces ')' followed by spaces with '), ' to maintain separation after exploding\n",
    "df = filtered_df.withColumn(\"normalized_skills\", regexp_replace(col(\"Skills\"), r\"\\)\\s+\", \"), \"))\n",
    "\n",
    "# Explode the skills based on ', ' after normalizing it to handle various formats\n",
    "df = df.withColumn(\"Skill\", explode(split(col(\"normalized_skills\"), \",\\s*\")))\n",
    "\n",
    "# Cleaning up skills by trimming spaces and converting to lowercase for uniformity\n",
    "df = df.withColumn(\"Skill\", lower(trim(col(\"Skill\"))))\n",
    "\n",
    "# Count the occurrences of each skill\n",
    "skill_counts = df.groupBy(\"Skill\").count().orderBy(col(\"count\").desc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c175b74f-69f6-45c5-81cd-d6bfe767a085",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:====================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               Skill|count|\n",
      "+--------------------+-----+\n",
      "|interaction desig...|20580|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "skill_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaede97b-1a9d-414f-85e0-9cc3fa866c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### feature 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d01961cc-cb16-4330-8c85-dbf3e8f2ba00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, lower, regexp_replace, array_contains, split\n",
    "from pyspark.sql.types import BooleanType\n",
    "\n",
    "df = spark.read.csv(\"job_descriptions.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# User profile\n",
    "user_profile = {\n",
    "    \"skills\": [\"HTML\", \"CSS\", \"JavaScript\", \"React\", \"Angular\", \"UX\"],\n",
    "    \"salary\": 100000\n",
    "}\n",
    "\n",
    "# Normalize skills in the dataset\n",
    "df = df.withColumn(\"Normalized_Skills\", lower(regexp_replace(col(\"Skills\"), \"[^a-zA-Z0-9\\s]\", \"\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74a95b44-a6ba-4b6a-a41e-ea220d331669",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `HTML` cannot be resolved. Did you mean one of the following? [`Role`, `Job Id`, `skills`, `Company`, `Contact`].;\n'Filter contains_skill(Normalized_Skills#605, array('HTML, 'CSS, 'JavaScript, 'React, 'Angular, 'UX))#631\n+- Project [Job Id#559L, Experience#560, Qualifications#561, Salary Range#562, location#563, Country#564, latitude#565, longitude#566, Work Type#567, Company Size#568, Job Posting Date#569, Preference#570, Contact Person#571, Contact#572, Job Title#573, Role#574, Job Portal#575, Job Description#576, Benefits#577, skills#578, Responsibilities#579, Company#580, Company Profile#581, lower(regexp_replace(Skills#578, [^a-zA-Z0-9\\s], , 1)) AS Normalized_Skills#605]\n   +- Relation [Job Id#559L,Experience#560,Qualifications#561,Salary Range#562,location#563,Country#564,latitude#565,longitude#566,Work Type#567,Company Size#568,Job Posting Date#569,Preference#570,Contact Person#571,Contact#572,Job Title#573,Role#574,Job Portal#575,Job Description#576,Benefits#577,skills#578,Responsibilities#579,Company#580,Company Profile#581] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m contains_skill_udf \u001b[38;5;241m=\u001b[39m udf(contains_skill, BooleanType())\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Apply the UDF to filter jobs by skills\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m df_filtered_by_skills \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontains_skill_udf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNormalized_Skills\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_profile\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mskills\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/bigdata/lib/python3.10/site-packages/pyspark/sql/dataframe.py:3325\u001b[0m, in \u001b[0;36mDataFrame.filter\u001b[0;34m(self, condition)\u001b[0m\n\u001b[1;32m   3323\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mfilter(condition)\n\u001b[1;32m   3324\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(condition, Column):\n\u001b[0;32m-> 3325\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcondition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3326\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   3328\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN_OR_STR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3329\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcondition\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(condition)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   3330\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/bigdata/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/envs/bigdata/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `HTML` cannot be resolved. Did you mean one of the following? [`Role`, `Job Id`, `skills`, `Company`, `Contact`].;\n'Filter contains_skill(Normalized_Skills#605, array('HTML, 'CSS, 'JavaScript, 'React, 'Angular, 'UX))#631\n+- Project [Job Id#559L, Experience#560, Qualifications#561, Salary Range#562, location#563, Country#564, latitude#565, longitude#566, Work Type#567, Company Size#568, Job Posting Date#569, Preference#570, Contact Person#571, Contact#572, Job Title#573, Role#574, Job Portal#575, Job Description#576, Benefits#577, skills#578, Responsibilities#579, Company#580, Company Profile#581, lower(regexp_replace(Skills#578, [^a-zA-Z0-9\\s], , 1)) AS Normalized_Skills#605]\n   +- Relation [Job Id#559L,Experience#560,Qualifications#561,Salary Range#562,location#563,Country#564,latitude#565,longitude#566,Work Type#567,Company Size#568,Job Posting Date#569,Preference#570,Contact Person#571,Contact#572,Job Title#573,Role#574,Job Portal#575,Job Description#576,Benefits#577,skills#578,Responsibilities#579,Company#580,Company Profile#581] csv\n"
     ]
    }
   ],
   "source": [
    "# Function to check if any user skill is in job skills\n",
    "from pyspark.sql.functions import udf, col, lower, regexp_replace, array_contains, split, array\n",
    "\n",
    "def contains_skill(job_skills, user_skills):\n",
    "    job_skills_list = job_skills.split(\" \")\n",
    "    return any(skill.lower() in job_skills_list for skill in user_skills)\n",
    "\n",
    "contains_skill_udf = udf(contains_skill, BooleanType())\n",
    "\n",
    "# Apply the UDF to filter jobs by skills\n",
    "df_filtered_by_skills = df.filter(contains_skill_udf(col(\"Normalized_Skills\"), array(user_profile['skills'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6c81a91-28da-40a0-86eb-2dc900e09513",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:====================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          Job Title|\n",
      "+-------------------+\n",
      "|   Graphic Designer|\n",
      "|      Web Developer|\n",
      "|Front-End Developer|\n",
      "| Software Developer|\n",
      "| Front-End Engineer|\n",
      "|       UI Developer|\n",
      "|  Software Engineer|\n",
      "+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col, lower, trim, regexp_replace, array, lit\n",
    "from pyspark.sql.types import BooleanType\n",
    "import re\n",
    "\n",
    "# UDF to check if any user skill is in job skills\n",
    "def contains_skill(job_skills, user_skills):\n",
    "    job_skills_list = job_skills.split(\" \")\n",
    "    return any(skill.lower() in job_skills_list for skill in user_skills)\n",
    "\n",
    "contains_skill_udf = udf(contains_skill, BooleanType())\n",
    "\n",
    "# Prepare a list of user skills as literal columns for comparison\n",
    "user_skills = [lit(skill.lower()) for skill in user_profile['skills']]\n",
    "\n",
    "# Normalize skills in the dataset\n",
    "df = df.withColumn(\"Normalized_Skills\", lower(trim(regexp_replace(col(\"Skills\"), \"[^a-zA-Z0-9\\s]\", \"\"))))\n",
    "\n",
    "# Filter jobs based on skills match\n",
    "df_filtered_by_skills = df.filter(contains_skill_udf(\"Normalized_Skills\", array(user_skills)))\n",
    "\n",
    "# Assuming a function or method to parse the salary\n",
    "# Function to parse salary and check range\n",
    "def salary_in_range(salary_range, desired_salary):\n",
    "    # Remove dollar signs and 'K', and split by '-'\n",
    "    salary_min, salary_max = [int(s) * 1000 for s in re.sub(r'[^\\d-]', '', salary_range).split('-')]\n",
    "    return salary_min <= desired_salary <= salary_max\n",
    "\n",
    "salary_udf = udf(salary_in_range, BooleanType())\n",
    "\n",
    "# Filter jobs where the salary fits the user's desired salary\n",
    "df_final = df_filtered_by_skills.filter(salary_udf(\"Salary Range\", lit(user_profile[\"salary\"])))\n",
    "\n",
    "# Select and display recommended job roles\n",
    "df_final.select(\"Job Title\").distinct().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a4bbd27-0737-40f0-955a-e424ff0a0fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"roles\": [\n",
      "        \"Graphic Designer\",\n",
      "        \"Web Developer\",\n",
      "        \"Front-End Developer\",\n",
      "        \"Software Developer\",\n",
      "        \"Front-End Engineer\",\n",
      "        \"UI Developer\",\n",
      "        \"Software Engineer\"\n",
      "    ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Assuming df_final contains the filtered DataFrame based on salary and skills\n",
    "job_titles = df_final.select(\"Job Title\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Construct the output dictionary\n",
    "output_dict = {\n",
    "    \"roles\": job_titles\n",
    "}\n",
    "\n",
    "# If you need a JSON string\n",
    "json_output = json.dumps(output_dict, indent=4)\n",
    "print(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1ae179-a6c8-4ab5-a741-1b6506ea6245",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bigdata]",
   "language": "python",
   "name": "conda-env-bigdata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
